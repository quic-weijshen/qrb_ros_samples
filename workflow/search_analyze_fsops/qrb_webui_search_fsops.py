#!/usr/bin/env python
# coding: utf-8

# <place_holder> 
# use ollama or any other AI API provider

# Tavily Search Tool hand crafted
# https://github.com/tavily-ai/tavily-python?tab=readme-ov-file#getting-and-printing-the-full-search-api-response
from tavily import TavilyClient

tavily_client = TavilyClient(api_key="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")


# Reference : https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-agent/nodes.py
# changed DuckDuckGo with Tavily
from pocketflow import Node, Flow
import yaml
import sys


# File System Node

def file_save(filename, content):
    try:
        with open(filename, "w", encoding="utf-8") as f:
            f.write(content)
        with open("latest.md", "a", encoding="utf-8") as lf:
            lf.write(content)
        return True
    except (FileNotFoundError, PermissionError, IsADirectoryError, OSError) as e:
        print(f"[File Write Fail] {type(e).__name__}: {e}", file=sys.stderr)
        return False

class LocalFileSystem(Node):
    def prep(self, shared):
        """Get the search result(context) from the shared store."""
        return shared["context"], shared["question"]
        
    def exec(self, inputs):
        """save search context into localfs for debug or building DataBase"""
        search_context, search_query = inputs
        
        # DEBUG 
        # print("- " * 40)
        # print(search_context)
        # print("- " * 40)
        # print(search_query)
        # print("- " * 40)
        
        # This is the prompt generated by LLM but prune by author with Q&A work model  
        prompt = f"""
### CONTEXT
You are a local file system assistant.  
You will parse given context and provide the best format to save the file.  
You can only use write access of your current path.  
You will aggregate **one round of search results into a single Markdown (.md) file** with:

***Highly-informative filename** for `ls` readability (topic, intent, count, key sources).
***High-density content** for `grep` (front matter metadata + normalized sections + source blocks).

### SEARCH QUERY
{search_query}

### SEARCH RESULT
{search_context}

#### Objectives

1.  Save **all search results of one query** into **one Markdown file**.
2.  Filename must allow another model to quickly infer content by reading only the filename.
3.  Content must maximize grep-ability: stable headings, consistent field labels, and explicit source blocks.

#### Filename Convention

**Examples**

*   `raspberry-pi-edge-agent-status-14results-wikipedia+forums.md`
*   `semantic-search-howto-9results-arxiv+github.md`

> If any component is missing (e.g., sources), omit that segment but keep order. Only use ASCII, `-`, `+`, and digits. No spaces.

#### File Write Policy

*   Write exactly **one** `.md` file in the current directory.
*   Do **not** overwrite an existing file with the same name; if collision occurs, append `-v2`, `-v3`, ‚Ä¶
*   Return the final absolute or relative path of the created file for downstream tools.
*   Provide the raw search result list or objects here; you may include `title`, `url`, `snippet`, `published_at`, `source`.
*   You must still normalize and expand them into the **Results** section above.
*   If the upstream already contains content from clicks, parse and use it.

### ACTION SPACE

\[1] file_save
Description: save file to local path
Parameters:
\- file_name (str): follow requested file name rules above
\- file_content (str): follow requested file content rules above

\[2] report_err
Description: the search context is empty or not applicable to save as file
Parameters:
\- None

## NEXT ACTION

Decide the next action based on the context and available actions.
Return your response in this format:

```yaml
thinking: |
    <your step-by-step reasoning process>
action: file_save OR report_err
reason: <why you chose this action>
error: <if action is report_err>
file_name: <specific file name if action is file_save>
file_content: <specific file content if action is file_save>
```

IMPORTANT: Make sure to:

1.  Use proper indentation (4 spaces) for all multi-line fields
2.  Use the | character for multi-line text fields
3.  Keep single-line fields without the | character

### END

"""
        # Call the LLM to make a decision
        response = call_llm(prompt)
        
        # Response should be yaml string already
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        file_system_dict = yaml.safe_load(yaml_str)
        
        # Call the search utility function
        print(f"File System Assistant : file name is {file_system_dict['file_name']}")
        
        written = file_save(file_system_dict["file_name"], file_system_dict["file_content"])
        if not written:
            exit(" X___X ")
        return file_system_dict
    
    def post(self, shared, prep_res, exec_res):
        """Save the search results and go back to the decision node."""
        # Add the local search file to the context in the shared store
        shared["local_search_file"] = open("latest.md", encoding="utf-8").read()

        shared["context"] = "\n\nSEARCH: " + shared["search_query"] + "\nRESULTS: " + shared["local_search_file"]
        
        print(f"üìö File System Node done, handling to DecideAction to analyze results...")
        
        # Always go back to the decision node after searching
        return "decide"

# Search Node

def search_web(query_str):
    response = tavily_client.search(query_str)
    results = response["results"]
    results_str = "\n\n".join([f"URL: {r['url']}\nTitle: {r['title']}\nContent: {r['content']}" for r in results])
    return results_str

class SearchWeb(Node):
    def prep(self, shared):
        """Get the search query from the shared store."""
        return shared["search_query"]
        
    def exec(self, search_query):
        """Search the web for the given query."""
        # Call the search utility function
        print(f"üåê Searching the web for: {search_query}")
        results = search_web(search_query)
        return results
    
    def post(self, shared, prep_res, exec_res):
        """Save the search results and go back to the decision node."""
        # Add the search results to the context in the shared store
        previous = shared.get("context", "")
        shared["context"] = previous + "\n\nSEARCH: " + shared["search_query"] + "\nRESULTS: " + exec_res
        
        print(f"üìö Found information, analyzing results...")
        
        # Always go back to the decision node after searching
        return "localfs"

# Decide Node

class DecideAction(Node):
    def prep(self, shared):
        """Prepare the context and question for the decision-making process."""
        # Get the current context (default to "No previous search" if none exists)
        context = shared.get("context", "No previous search")
        # Get the question from the shared store
        question = shared["question"]
        # Return both for the exec step
        return question, context
        
    def exec(self, inputs):
        """Call the LLM to decide whether to search or answer."""
        question, context = inputs
        
        #fetch timestamp
        ts = __import__('datetime').datetime.now().isoformat()
        
        print(f"ü§î Agent deciding what to do next...")
        
        # Create a prompt to help the LLM decide what to do next with proper yaml formatting
        # be careful to construct your prompt to achieve best efficiency
        prompt = f"""
### CONTEXT
You are a research assistant that can search the web.
Current Time:{ts}
Question: {question}
Previous Research: {context}

### ACTION SPACE
[1] search
  Description: Look up more information on the web
  Parameters:
    - query (str): What to search for

[2] answer
  Description: Answer the question with current knowledge
  Parameters:
    - answer (str): Final answer to the question

## NEXT ACTION
Decide the next action based on the context and available actions.
Return your response in this format:

```yaml
thinking: |
    <your step-by-step reasoning process>
action: search OR answer
reason: <why you chose this action>
answer: <if action is answer>
search_query: <specific search query if action is search>
```
IMPORTANT: Make sure to:
1. Use proper indentation (4 spaces) for all multi-line fields
2. Use the | character for multi-line text fields
3. Keep single-line fields without the | character

### END

"""
        # Call the LLM to make a decision
        response = call_llm(prompt)
        
        # Parse the response to get the decision
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()
        decision = yaml.safe_load(yaml_str)
        
        return decision
    
    def post(self, shared, prep_res, exec_res):
        """Save the decision and determine the next step in the flow."""
        # If LLM decided to search, save the search query
        if exec_res["action"] == "search":
            shared["search_query"] = exec_res["search_query"]
            print(f"üîç Agent decided to search for: {exec_res['search_query']}")
        else:
            shared["context"] = exec_res["answer"] #save the context if LLM gives the answer without searching.
            print(f"üí° Agent decided to answer the question")
        
        # Return the action to determine the next node in the flow
        return exec_res["action"]

# Answer Node

class AnswerQuestion(Node):
    def prep(self, shared):
        """Get the question and context for answering."""
        return shared["question"], shared.get("context", "")
        
    def exec(self, inputs):
        """Call the LLM to generate a final answer."""
        question, context = inputs
        
        print(f"‚úçÔ∏è Crafting final answer...")
        
        # Create a prompt for the LLM to answer the question
        prompt = f"""
### CONTEXT
Based on the following information, answer the question.
Question: {question}
Research: {context}

## YOUR ANSWER:
Provide a comprehensive answer using the research results.
"""
        # Call the LLM to generate an answer
        answer = call_llm(prompt)
        return answer
    
    def post(self, shared, prep_res, exec_res):
        """Save the final answer and complete the flow."""
        # Save the answer in the shared store
        shared["answer"] = exec_res
        
        print(f"‚úÖ Answer generated successfully")
        
        # We're done - no need to continue the flow
        return "done" 

from pocketflow import Flow

def create_agent_flow():
    """
    Create and connect the nodes to form a complete agent flow.
    
    The flow works like this:
    1. DecideAction node decides whether to search or answer
    2. If search, go to SearchWeb node, and then go to LocalFileSystem node
    3. If answer, go to AnswerQuestion node
    4. After SearchWeb completes, go back to DecideAction
    
    Returns:
        Flow: A complete research agent flow
    """
    # Create instances of each node
    decide = DecideAction()
    search = SearchWeb()
    answer = AnswerQuestion()
    localfs = LocalFileSystem()

    # Connect the nodes
    # If DecideAction returns "search", go to SearchWeb
    decide - "search" >> search
    
    # If DecideAction returns "answer", go to AnswerQuestion
    decide - "answer" >> answer
    
    # After SearchWeb completes and returns "decide", go back to DecideAction
    search - "localfs" >> localfs
    localfs - "decide" >> decide

    # Create and return the flow, starting with the DecideAction node
    return Flow(start=decide)

# Reference : https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-agent/flow.py
# Reference : https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow-agent/main.py
import sys

def main():
    """Simple function to process a question."""
    
    # Default question, Solute to David Tao
    default_question = "I want latest news of David Tao"

    # Get question from command line if provided with --
    question = default_question
    for arg in sys.argv[1:]:
        if arg.startswith("--"):
            question = arg[2:]
            break

    # Create the agent flow
    agent_flow = create_agent_flow()

    # Process the question
    shared = {"question": question}
    print(f"ü§î Processing question: {question}")
    agent_flow.run(shared)
    print("\nüéØ Final Answer:")
    res = shared.get("answer", "No answer found")
    print(res)

main()